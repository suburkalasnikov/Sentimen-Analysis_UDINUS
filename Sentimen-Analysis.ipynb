#import modul
import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from time import sleep
import getpass as gp
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
from googletrans import Translator  # Import the Translator class
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt


all_tweets = []
translator = Translator()

# Membuat DataFrame untuk menyimpan data
df = pd.DataFrame(columns=['usertag', 'timestamp', 'content', 'replies', 'retweets', 'likes'])

# Mulai melakukan scraping
while True:
    tweets = driver.find_elements(By.XPATH, "//div[@data-testid='tweetText']")
    for tweet in tweets:
        try:
            tweet_text = tweet.text
            usertag = driver.find_element(By.XPATH, './/div[@data-testid="User-Name"]//span').text
            timestamp = driver.find_element(By.XPATH, './/time').get_attribute('datetime')
            reply_count =driver.find_element(By.XPATH, './/div[@data-testid="reply"]//span').text
            retweet_count = driver.find_element(By.XPATH, './/div[@data-testid="retweet"]//span').text
            like_count = driver.find_element(By.XPATH, './/div[@data-testid="like"]//span').text
            
            # Translate tweet text to English
            translation = translator.translate(tweet_text, dest='en')
            
            # Append data to the DataFrame
            df = pd.concat([df, pd.DataFrame({
                'usertag': [usertag],
                'timestamp': [timestamp],
                'content': [translation.text],
                'replies': [reply_count],
                'retweets': [retweet_count],
                'likes': [like_count]
            })], ignore_index=True)

            all_tweets.append(translation.text)
            print(translation.text)
        except Exception as e:
            print(f"Error processing tweet: {e}")

    # Scroll down to load more tweets
    driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
    sleep(3)

    if len(all_tweets) >100:  # Adjust the number of tweets as needed
        break

# Save the DataFrame to a CSV file
df.to_csv('Sentimen.csv', index=False)

df.head()
df.tail()
